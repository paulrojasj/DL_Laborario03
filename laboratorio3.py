# -*- coding: utf-8 -*-
"""Laboratorio3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1COFSB7yKAGW_ppAkIXO9RNclEryPImsQ
"""

!pip install openai-whisper # NO whisper
!pip install jiwer
!pip -q install transformers accelerate sentencepiece
!pip install coqui-tts # NO tts

"""#  Grabar audio"""

from IPython.display import Javascript, HTML, display, Audio
from google.colab import output
import base64, subprocess, uuid, os, pathlib

# @title Grabación de audio
# @markdown Lo necesitas para grabar audio desde Colab. Intenta mejorarlo.
def record(out_webm=None, out_wav=None, sr=16000, autoplay=False):
    if out_webm is None:
        out_webm = f"/content/rec_{uuid.uuid4().hex}.webm"
    if out_wav is None:
        out_wav  = f"/content/rec_{uuid.uuid4().hex}.wav"

    js = Javascript(r"""
    async function recorderUI(){
      // Contenedor visual
      const box = document.createElement('div');
      box.style.padding = '12px';
      box.style.margin = '8px 0';
      box.style.border = '1px solid #ddd';
      box.style.borderRadius = '10px';
      box.style.display = 'inline-flex';
      box.style.gap = '8px';
      box.style.alignItems = 'center';
      box.style.fontFamily = 'sans-serif';

      const recDot = document.createElement('span');
      recDot.style.width = '10px';
      recDot.style.height = '10px';
      recDot.style.borderRadius = '50%';
      recDot.style.background = '#bbb';

      const msg = document.createElement('span');
      msg.textContent = 'Listo para grabar';
      msg.style.minWidth = '180px';

      const btnStart = document.createElement('button');
      btnStart.textContent = 'Grabar';
      btnStart.style.padding = '6px 10px';

      const btnStop = document.createElement('button');
      btnStop.textContent = 'Parar';
      btnStop.style.padding = '6px 10px';
      btnStop.disabled = true;

      box.appendChild(recDot);
      box.appendChild(btnStart);
      box.appendChild(btnStop);
      box.appendChild(msg);
      document.body.appendChild(box);

      let stream = null;
      let rec = null;
      let chunks = [];

      function setRecording(on){
        recDot.style.background = on ? '#e74c3c' : '#bbb';
        msg.textContent = on ? 'Grabando...' : 'Listo para grabar';
        btnStart.disabled = on;
        btnStop.disabled  = !on;
      }

      return await new Promise(async (resolve, reject) => {
        try{
          stream = await navigator.mediaDevices.getUserMedia({audio:true});
          rec = new MediaRecorder(stream);
        }catch(err){
          box.remove();
          reject('No se pudo acceder al micrófono: ' + err);
          return;
        }

        rec.ondataavailable = e => { if (e.data && e.data.size > 0) chunks.push(e.data); };

        rec.onstop = async () => {
          try{
            const blob = new Blob(chunks, {type:'audio/webm;codecs=opus'});
            const buf = await blob.arrayBuffer();
            const b64 = btoa(String.fromCharCode(...new Uint8Array(buf)));
            stream.getTracks().forEach(t => t.stop());
            box.remove();
            resolve(b64);
          }catch(err){
            stream.getTracks().forEach(t => t.stop());
            box.remove();
            reject(err);
          }
        };

        btnStart.onclick = () => { chunks = []; rec.start(); setRecording(true); };
        btnStop.onclick  = () => { if(rec && rec.state === 'recording'){ rec.stop(); setRecording(false); } };
      });
    }
    """)
    display(js)

    b64 = output.eval_js("recorderUI()")
    with open(out_webm, "wb") as f:
      f.write(base64.b64decode(b64))

    subprocess.run([
        "ffmpeg","-y","-i",out_webm,"-ac","1","-ar",str(sr),out_wav
    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    try:
        os.remove(out_webm)
    except FileNotFoundError:
        pass

    display(Audio(filename=out_wav, autoplay=autoplay))
    return out_wav

# Ejecuta el programa
wav_path = record(out_wav="/content/input.wav", autoplay=True)

import gc
import torch

gc.collect()
torch.cuda.empty_cache()

"""# Automatic speech recognition"""

import whisper, time
from jiwer import wer

MODEL = "turbo"   # opciones disponibles: tiny, base, small, medium, large, turbo
m = whisper.load_model(MODEL)
audio_in = "input.wav"

t0 = time.time()
out = m.transcribe(audio_in, language="en", task="transcribe", verbose=False)
t1 = time.time()

user_text = out["text"].strip()
print("ASR:", user_text)
print(f"Latency: {t1 - t0:.2f}s")

"""# LLM

Puedes usar la configuración por defecto, pero te animo a probar otras configuraciones!
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model_id = "google/flan-t5-base"
tok = AutoTokenizer.from_pretrained(model_id)
mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map="auto")

pipe = pipeline("text2text-generation", model=mdl, tokenizer=tok)
prompt = f"Eres un asistente útil. Responde en 1-2 oraciones y sé directo. Pregunta: {user_text}"
resp = pipe(prompt, max_new_tokens=128)[0]["generated_text"].strip()
print("LLM:", resp)

"""# TTS"""

from TTS.api import TTS
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")

print(tts.speakers)

tts.tts_to_file(
    text=resp,
    # speaker_wav="ref_voice.wav",    # tu audio de referencia (5–10 s)
                                      # procura que sea audio de calidad
    speaker = 'Claribel Dervla',
    language="es",
    file_path="respuesta.wav"
)

display(Audio(filename="respuesta.wav", autoplay=True))