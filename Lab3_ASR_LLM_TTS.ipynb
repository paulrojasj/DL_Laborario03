{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 — ASR + LLM + TTS end-to-end\n\nEste cuaderno implementa el flujo ASR → LLM → TTS solicitado en el Laboratorio 3. Incluye grabación de audio en Colab, transcripción con Whisper, generación de respuesta con un LLM local, síntesis con xTTS (coqui-tts) y un resumen de latencias por etapa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalación de dependencias\n\nEjecuta esta celda una sola vez por sesión para instalar las librerías requeridas. Se utiliza `coqui-tts` (no el paquete `tts`) tal como solicita la guía.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai-whisper jiwer\n",
    "!pip install -q transformers accelerate sentencepiece\n",
    "!pip install -q coqui-tts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grabación de audio en Colab\n\nSe reutiliza el widget provisto por el profesor para capturar audio desde el navegador. Puedes volver a ejecutarlo cada vez que necesites una nueva muestra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript, HTML, display, Audio\n",
    "from google.colab import output\n",
    "import base64, subprocess, uuid, os\n",
    "from pathlib import Path\n",
    "\n",
    "# @title Widget de grabación compatible con Colab\n",
    "def record(out_webm=None, out_wav=None, sr=16000, autoplay=False):\n",
    "    \"\"\"Graba audio desde el navegador y devuelve la ruta del WAV generado.\"\"\"\n",
    "    if out_webm is None:\n",
    "        out_webm = f\"/content/rec_{uuid.uuid4().hex}.webm\"\n",
    "    if out_wav is None:\n",
    "        out_wav = f\"/content/rec_{uuid.uuid4().hex}.wav\"\n",
    "\n",
    "    js = Javascript(r\"\"\"\n",
    "    async function recorderUIOnce(){\n",
    "      const existing = document.getElementById('recorder-box');\n",
    "      if (existing) { existing.remove(); }\n",
    "\n",
    "      const box = document.createElement('div');\n",
    "      box.id = 'recorder-box';\n",
    "      box.style.cssText = 'padding:12px;margin:8px 0;border:1px solid #ddd;border-radius:10px;display:inline-flex;gap:8px;align-items:center;font-family:sans-serif';\n",
    "\n",
    "      const dot = document.createElement('span');\n",
    "      dot.style.cssText = 'width:10px;height:10px;border-radius:50%;background:#bbb';\n",
    "\n",
    "      const startBtn = document.createElement('button');\n",
    "      startBtn.textContent = 'Grabar';\n",
    "      startBtn.style.cssText = 'padding:6px 10px';\n",
    "\n",
    "      const stopBtn = document.createElement('button');\n",
    "      stopBtn.textContent = 'Parar';\n",
    "      stopBtn.style.cssText = 'padding:6px 10px';\n",
    "      stopBtn.disabled = true;\n",
    "\n",
    "      const msg = document.createElement('span');\n",
    "      msg.textContent = 'Listo para grabar';\n",
    "      msg.style.minWidth = '180px';\n",
    "\n",
    "      box.append(dot, startBtn, stopBtn, msg);\n",
    "      document.body.appendChild(box);\n",
    "\n",
    "      let stream, rec, chunks = [];\n",
    "\n",
    "      function setRec(on){\n",
    "        dot.style.background = on ? '#e74c3c' : '#bbb';\n",
    "        startBtn.disabled = on;\n",
    "        stopBtn.disabled = !on;\n",
    "        msg.textContent = on ? 'Grabando…' : 'Listo para grabar';\n",
    "      }\n",
    "\n",
    "      return await new Promise(async (resolve, reject) => {\n",
    "        try {\n",
    "          stream = await navigator.mediaDevices.getUserMedia({ audio:true });\n",
    "          rec = new MediaRecorder(stream);\n",
    "        } catch (err) {\n",
    "          box.remove();\n",
    "          reject('No se pudo acceder al micrófono: ' + err);\n",
    "          return;\n",
    "        }\n",
    "\n",
    "        rec.ondataavailable = e => { if (e.data && e.data.size > 0) chunks.push(e.data); };\n",
    "\n",
    "        rec.onstop = async () => {\n",
    "          try {\n",
    "            const blob = new Blob(chunks, {type:'audio/webm;codecs=opus'});\n",
    "            const buf = await blob.arrayBuffer();\n",
    "            const b64 = btoa(String.fromCharCode(...new Uint8Array(buf)));\n",
    "            stream.getTracks().forEach(t => t.stop());\n",
    "            box.remove();\n",
    "            resolve(b64);\n",
    "          } catch (err) {\n",
    "            stream.getTracks().forEach(t => t.stop());\n",
    "            box.remove();\n",
    "            reject(err);\n",
    "          }\n",
    "        };\n",
    "\n",
    "        startBtn.onclick = () => { chunks = []; rec.start(); setRec(true); };\n",
    "        stopBtn.onclick = () => {\n",
    "          if (rec && rec.state === 'recording') {\n",
    "            rec.stop();\n",
    "            setRec(false);\n",
    "          }\n",
    "        };\n",
    "      });\n",
    "    }\n",
    "    \"\"\")\n",
    "    display(js)\n",
    "\n",
    "    b64 = output.eval_js(\"recorderUIOnce()\")\n",
    "    with open(out_webm, 'wb') as f:\n",
    "        f.write(base64.b64decode(b64))\n",
    "\n",
    "    subprocess.run([\n",
    "        'ffmpeg', '-y', '-i', out_webm, '-ac', '1', '-ar', str(sr), out_wav\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    try:\n",
    "        os.remove(out_webm)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    display(Audio(filename=out_wav, autoplay=autoplay))\n",
    "    return out_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Graba la pregunta para el pipeline\n",
    "QUESTION_WAV = record(out_wav='/content/input_question.wav', autoplay=True)\n",
    "print('Pregunta guardada en:', QUESTION_WAV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestra de voz para clonación (opcional)\n\nGraba o carga una muestra de voz (5–10 segundos) que represente al hablante. Esta muestra se utilizará como `speaker_wav` para xTTS y comparar frente a la voz base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title (Opcional) Graba la voz de referencia para clonación\n",
    "# @markdown Ejecuta esta celda si deseas capturar una voz de referencia directamente en Colab.\n",
    "CLONE_REFERENCE_WAV = record(out_wav='/content/voice_clone_ref.wav', autoplay=True)\n",
    "print('Voz de referencia guardada en:', CLONE_REFERENCE_WAV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa: sube un archivo WAV/MP3 existente desde tu computadora\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# CLONE_REFERENCE_WAV = next(iter(uploaded.keys()))  # usa la primera clave subida\n",
    "# print('Archivo de referencia cargado:', CLONE_REFERENCE_WAV)\n",
    "\n",
    "# Si ya conoces la ruta manualmente, puedes asignarla así:\n",
    "# CLONE_REFERENCE_WAV = '/content/mi_referencia.wav'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic Speech Recognition (ASR)\n\nUsamos Whisper para transcribir el audio grabado. Se reporta la latencia y el texto reconocido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ASR_MODEL_NAME = 'turbo'  # puedes cambiarlo por tiny, base, small, medium o large\n",
    "whisper_model = whisper.load_model(ASR_MODEL_NAME)\n",
    "print('Modelo Whisper cargado:', ASR_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, *, language='es', task='transcribe', model=whisper_model):\n",
    "    if not audio_path:\n",
    "        raise ValueError('Debes proporcionar la ruta del audio a transcribir.')\n",
    "    start = time.perf_counter()\n",
    "    result = model.transcribe(audio_path, language=language, task=task, verbose=False)\n",
    "    latency = time.perf_counter() - start\n",
    "    text = result['text'].strip()\n",
    "    return text, latency, result\n",
    "\n",
    "TRANSCRIPT_TEXT, ASR_LATENCY, ASR_RAW = transcribe_audio(QUESTION_WAV, language='es')\n",
    "print('ASR:', TRANSCRIPT_TEXT)\n",
    "print(f'Latencia ASR: {ASR_LATENCY:.2f} s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM — Generación de respuesta corta\n\nSe utiliza FLAN-T5 (puedes cambiarlo por otro modelo disponible en HuggingFace). Se mide la latencia y se limita la respuesta a pocas oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "LLM_MODEL_ID = 'google/flan-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_ID, device_map='auto')\n",
    "text_generator = pipeline('text2text-generation', model=llm_model, tokenizer=tokenizer)\n",
    "print('Modelo LLM cargado:', LLM_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    'Eres un asistente útil. Responde en 1-2 oraciones y sé directo. '\n",
    "    'Pregunta: {transcript}'\n",
    ")\n",
    "\n",
    "def generate_reply(transcript, *, max_new_tokens=128):\n",
    "    prompt = PROMPT_TEMPLATE.format(transcript=transcript)\n",
    "    start = time.perf_counter()\n",
    "    output = text_generator(prompt, max_new_tokens=max_new_tokens)\n",
    "    latency = time.perf_counter() - start\n",
    "    reply = output[0]['generated_text'].strip()\n",
    "    return reply, latency, prompt\n",
    "\n",
    "RESPONSE_TEXT, LLM_LATENCY, LAST_PROMPT = generate_reply(TRANSCRIPT_TEXT)\n",
    "print('LLM:', RESPONSE_TEXT)\n",
    "print(f'Latencia LLM: {LLM_LATENCY:.2f} s')\n",
    "print('Prompt usado:', LAST_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TTS — Síntesis con Coqui TTS (xTTS)\n\nSe carga el modelo `tts_models/multilingual/multi-dataset/xtts_v2`. Se sintetiza la respuesta con una voz base y, si se proporciona `CLONE_REFERENCE_WAV`, también con la voz clonada para comparar la calidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "tts_model = TTS('tts_models/multilingual/multi-dataset/xtts_v2')\n",
    "AVAILABLE_SPEAKERS = tts_model.speakers or []\n",
    "print('Voces base disponibles:', AVAILABLE_SPEAKERS)\n",
    "BASE_SPEAKER = AVAILABLE_SPEAKERS[0] if AVAILABLE_SPEAKERS else None\n",
    "print('Voz base seleccionada:', BASE_SPEAKER)\n",
    "LANGUAGE_CODE = 'es'  # cambia a 'en', 'pt', etc. si trabajas en otro idioma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def synthesize_with_xtts(text, *, output_path, speaker=None, speaker_wav=None, language='es'):\n",
    "    start = time.perf_counter()\n",
    "    kwargs = {\n",
    "        'text': text,\n",
    "        'language': language,\n",
    "        'file_path': str(output_path),\n",
    "    }\n",
    "    if speaker:\n",
    "        kwargs['speaker'] = speaker\n",
    "    if speaker_wav:\n",
    "        kwargs['speaker_wav'] = speaker_wav\n",
    "    tts_model.tts_to_file(**kwargs)\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "BASE_TTS_PATH = Path('/content/respuesta_base.wav')\n",
    "CLONE_TTS_PATH = Path('/content/respuesta_clonada.wav')\n",
    "\n",
    "print('Rutas de salida configuradas:')\n",
    "print('  Base:', BASE_TTS_PATH)\n",
    "print('  Clonada:', CLONE_TTS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline integrado y métricas\n\nLa siguiente función ejecuta el pipeline completo (ASR → LLM → TTS), guarda los audios generados y muestra las latencias por etapa junto con el tiempo total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(audio_path, *, asr_language='es', tts_language='es', base_speaker=BASE_SPEAKER, speaker_wav=None, max_new_tokens=128):\n",
    "    if not audio_path:\n",
    "        raise ValueError('Debes grabar o proporcionar un audio de entrada (QUESTION_WAV).')\n",
    "    summary = []\n",
    "    overall_start = time.perf_counter()\n",
    "\n",
    "    transcript, asr_latency, _ = transcribe_audio(audio_path, language=asr_language)\n",
    "    summary.append({'Etapa': 'ASR (Whisper)', 'Latencia (s)': asr_latency, 'Detalle': f'{len(transcript.split())} palabras'})\n",
    "    print('Transcripción:', transcript)\n",
    "\n",
    "    reply, llm_latency, prompt = generate_reply(transcript, max_new_tokens=max_new_tokens)\n",
    "    summary.append({'Etapa': 'LLM (FLAN-T5)', 'Latencia (s)': llm_latency, 'Detalle': f'{len(reply.split())} palabras'})\n",
    "    print('Respuesta generada:', reply)\n",
    "\n",
    "    print('\n",
    "▶️ Síntesis con voz base')\n",
    "    base_latency = synthesize_with_xtts(reply, output_path=BASE_TTS_PATH, speaker=base_speaker, language=tts_language)\n",
    "    summary.append({'Etapa': 'TTS voz base', 'Latencia (s)': base_latency, 'Detalle': base_speaker or 'default'})\n",
    "    display(Audio(filename=str(BASE_TTS_PATH), autoplay=False))\n",
    "\n",
    "    clone_path = None\n",
    "    if speaker_wav and Path(speaker_wav).exists():\n",
    "        print('\n",
    "🎯 Síntesis con voz clonada')\n",
    "        clone_latency = synthesize_with_xtts(reply, output_path=CLONE_TTS_PATH, speaker_wav=speaker_wav, language=tts_language)\n",
    "        summary.append({'Etapa': 'TTS voz clonada', 'Latencia (s)': clone_latency, 'Detalle': Path(speaker_wav).name})\n",
    "        clone_path = str(CLONE_TTS_PATH)\n",
    "        display(Audio(filename=clone_path, autoplay=False))\n",
    "    else:\n",
    "        print('\n",
    "No se encontró speaker_wav; se omite la comparación de voz clonada.')\n",
    "\n",
    "    total_latency = time.perf_counter() - overall_start\n",
    "    summary.append({'Etapa': 'Total pipeline', 'Latencia (s)': total_latency, 'Detalle': ''})\n",
    "\n",
    "    metrics_df = pd.DataFrame(summary)\n",
    "    display(metrics_df)\n",
    "\n",
    "    return {\n",
    "        'transcript': transcript,\n",
    "        'reply': reply,\n",
    "        'prompt': prompt,\n",
    "        'metrics': metrics_df,\n",
    "        'base_audio': str(BASE_TTS_PATH),\n",
    "        'clone_audio': clone_path,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_wav = None\n",
    "if 'CLONE_REFERENCE_WAV' in globals():\n",
    "    clone_wav = CLONE_REFERENCE_WAV\n",
    "\n",
    "results = run_pipeline(\n",
    "    QUESTION_WAV,\n",
    "    asr_language='es',\n",
    "    tts_language=LANGUAGE_CODE,\n",
    "    base_speaker=BASE_SPEAKER,\n",
    "    speaker_wav=clone_wav,\n",
    "    max_new_tokens=96,\n",
    ")\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}